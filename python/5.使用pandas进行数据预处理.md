## 合并数据

### 堆叠并和数据

#### 横向堆叠

使用concat函数完成

当参数axis=1时，concat作行对齐；即横向堆叠

join参数可以接受inner或outer，在内连接的情况下，仅仅返回索引重叠部分；在外连接的情况下，则显示索引的并集部分数据，不足的地方则使用控制填充。

```python
import numpy as np
import pandas as pd
from sqlalchemy import create_engine
conn = create_engine('mysql+pymysql://root:460204715@127.0.0.1:3306/python_db?charset=utf8')
detail1 = pd.read_sql('meal_order_detail1',conn)
df1 = detail1.iloc[:,:10] ##取出detail1的前10列数据
df2 = detail1.iloc[:,10:] ##取出detail1的后9列数据
print('合并df1的大小为%s，df2的大小为%s。'%(df1.shape,df2.shape))
print('外连接合并后的数据框大小为：',pd.concat([df1,df2],
        axis=1,join='inner').shape)
print('内连接合并后的数据框大小为：',pd.concat([df1,df2],
        axis=1,join='outer').shape)

//output:
合并df1的大小为(2779, 10)，df2的大小为(2779, 9)。
外连接合并后的数据框大小为： (2779, 19)
内连接合并后的数据框大小为： (2779, 19)
```

#### 纵向堆叠

concat函数和append方法均可实现

使用concat函数时，在默认情况下，即axis=0时，concat作列对齐。

在两张表的列名并不完全相同的情况下，可以使用join参数；取值为inner时，返回的仅仅是列名的交集所代表的列；取值为outer时，返回的是两者列名的并集所代表的列。

```python
df3 = detail1.iloc[:1500,:] ##取出detail1前1500行数据
df4 = detail1.iloc[1500:,:] ##取出detail1的1500后的数据
print('合并df3的大小为%s，df4的大小为%s。'%(df3.shape,df4.shape))
print('内连接纵向合并后的数据框大小为：',pd.concat([df3,df4],
        axis=1,join='inner').shape)
print('外连接纵向合并后的数据框大小为：',pd.concat([df3,df4],
        axis=1,join='outer').shape)
```

append方法使用条件：两张表的列名需要完全一致

```python
print('堆叠前df3的大小为%s，df4的大小为%s。'%(df3.shape,df4.shape))
print('append纵向堆叠后的数据框大小为：',df3.append(df4).shape)

//output:
堆叠前df3的大小为(1500, 19)，df4的大小为(1279, 19)。
append纵向堆叠后的数据框大小为： (2779, 19)
```

### 主键合并数据

pandas库中的merge函数和join方法都可以实现主键合并，但两者的实现方式并不相同。

merge函数连接方式：left\right\inner\outer

```python
order = pd.read_csv('meal_order_info.csv',
        sep=',',encoding='gb18030') ##读取订单信息表
##info_id转换为字符串格式，为合并做准备
order['info_id'] = order['info_id'].astype('str') 
## 订单详情表和订单信息表都有订单编号
##在订单详情表中为order_id，在订单信息表中为info_id
order_detail = pd.merge(detail1,order,
        left_on='order_id',right_on = 'info_id')
print('detail1订单详情表的原始形状为：',detail1.shape)
print('order订单信息表的原始形状为：',order.shape)
print('订单详情表和订单信息表主键合并后的形状为：',order_detail.shape)
```

join方法也可以实现部分主键合并的功能，但是使用此方法时，需要两个主键的方法名字相同：

```python
order.rename({'info_id':'order_id'},inplace=True)
order_detail1 = detail1.join(order,on='order_id',rsuffix='1')
print('订单详情表和订单信息表join合并后的形状为：',order_detail1.shape)
```

### 重叠合并数据

pandas.DataFrame.combine_first

```python
##建立两个字典，除了ID外，别的特征互补
dict1 = {'ID':[1,2,3,4,5,6,7,8,9],
         'System':['win10','win10',np.nan,'win10',
                np.nan,np.nan,'win7','win7','win8'],
      'cpu':['i7','i5',np.nan,'i7',np.nan,np.nan,'i5','i5','i3']}

dict2 = {'ID':[1,2,3,4,5,6,7,8,9],
         'System':[np.nan,np.nan,'win7',np.nan,
                'win8','win7',np.nan,np.nan,np.nan],
        'cpu':[np.nan,np.nan,'i3',np.nan,'i7',
                'i5',np.nan,np.nan,np.nan]}
## 转换两个字典为DataFrame
df5 = pd.DataFrame(dict1)
df6 = pd.DataFrame(dict2)
print('经过重叠合并后的数据为：\n',df5.combine_first(df6))

//output:
经过重叠合并后的数据为：
    ID System cpu
0   1  win10  i7
1   2  win10  i5
2   3   win7  i3
3   4  win10  i7
4   5   win8  i7
5   6   win7  i5
6   7   win7  i5
7   8   win7  i5
8   9   win8  i3
```

## 清洗数据

### 检测与处理重复值

常用的数据重复分为两种：一种为记录重复，即一个或者多个特征的某几条记录的值完全相同；另一种为特征重复，即存在一个或者多个特征名称不同，但数据完全相同的情况。

#### 记录重复

方法一：利用列表list去重

```python
import pandas as pd
detail = pd.read_csv('detail.csv',
    index_col=0,encoding = 'gbk')

##方法一
##定义去重函数
def delRep(list1):
    list2=[]
    for i in list1:
        if i not in list2:
            list2.append(i)
    return list2 
## 去重
dishes=list(detail['dishes_name']) ##将dishes_name从数据框中提取出来
print('去重前菜品总数为：',len(dishes)) 
dish = delRep(dishes) ##使用自定义的去重函数去重
print('方法一去重后菜品总数为：',len(dish))
```

方法二：利用集合set元素唯一的特性去重

```python
print('去重前菜品总数为：',len(dishes)) 
dish_set = set(dishes) ##利用set的特性去重
print('方法二去重后菜品总数为：',len(dish_set))
```

方法二代码简洁了很多，但是导致数据的排序顺序发生改变。

pandas提供了一个名为drop_duplicates的去重方法。该方法只能对DataFrame或者Series类型有效。这种方法不会改变数据原始排序：

```python
dishes_name = detail['dishes_name'].drop_duplicates()
print('drop_duplicates方法去重之后菜品总数为：',len(dishes_name))
```

drop_duplicates不仅支持单一特征的数据去重，还能依据DataFrame的其中一个或几个特征进行去重操作

```python
print('去重之前订单详情表的形状为：', detail.shape)
shapeDet = detail.drop_duplicates(subset = ['order_id',
    'emp_id']).shape
print('依照订单编号，会员编号去重之后订单详情表大小为:', shapeDet)
```

#### 特征重复

可以利用特征相似度将两个相似度为1的特征去除一个。在pandas中，相似度的计算方法为corr。

使用该方法计算相似度时，默认为person法，可以通过method参数调节，目前还支持spearman法和kendall法。

```python
corrDet = detail[['counts','amounts']].corr(method='kendall')
print('销量和售价的kendall相似度为：\n',corrDet)
```

该方法只能对数值型重复特征去重，类别型特征之间无法通过计算相似矩阵系数来衡量相似度，因此无法根据相度矩阵对齐进行去重处理。

除了使用相似度矩阵进行特征去重之外，还可以通过DataFrame.equals方法进行特征去重

```python
# 定义求取特征是否完全相同的矩阵的函数
def FeatureEquals(df):
    dfEquals=pd.DataFrame([],columns=df.columns,index=df.columns)
    for i in df.columns:
       for j in df.columns:
        	# 进行特征对比，完全相同返回true；反之返回false
           dfEquals.loc[i,j]=df.loc[:,i].equals(df.loc[:,j])
    return dfEquals
## 应用上述函数
detEquals=FeatureEquals(detail)
print('detail的特征相等矩阵的前5行5列为：\n',detEquals.iloc[:5,:5])
```

通过遍历的方式进行数据筛选

```python
# 获得列数
lenDet = detEquals.shape[0]
dupCol = []
for k in range(lenDet):
    for l in range(k+1,lenDet):
        # 如果布尔矩阵为true，即特征完全相同且duploc并没有此列，故添加此列
        if detEquals.iloc[k,l] & (detEquals.columns[l] not in dupCol):
            dupCol.append(detEquals.columns[l])
##进行去重操作
print('需要删除的列为：',dupCol)
detail.drop(dupCol,axis=1,inplace=True)
print('删除多余列后detail的特征数目为：',detail.shape[1])
```

### 检测与处理缺失值

数据中某个或某些特征的值是不完整的，这些值称为缺失值。可以使用pandas提供的识别缺失值的方法isnull和识别非缺失值的方法notnull

```python
print('detail每个特征缺失的数目为：\n',detail.isnull().sum())
print('detail每个特征非缺失的数目为：\n',detail.notnull().sum())
```

#### 删除法

删除法是将含有缺失值的特征或记录删除。分为删除观测记录和删除特征两种。

pandas提供了简单的缺失值的方法dropna，通过参数控制，既可以删除观测记录，也可以删除特征。

```python
print('去除缺失的列前detail的形状为：', detail.shape)
# how：any表示只要有缺失值就删除；all比哦啊哈斯当且仅当全部为缺失值才删除
print('去除缺失的列后detail的形状为：',
    detail.dropna(axis = 1,how ='any').shape)
```

#### 替换法

用一个特定的值替换缺失值。

分为数值型和类别型。

* 数值型：通常利用其均值、中位数、众数等统计量来代替缺失值
* 类别是：通常利用众数来替换缺失值

pandas提供了缺失值替换的方法fillna

```python
detail = detail.fillna(-99)
print('detail每个特征缺失的数目为：\n',detail.isnull().sum())
```

#### 插值法

常用的插值法有：

* 线性插值：针对已有的值求出线程方程，通过线性方程得到缺失值
* 多项式插值：用已知的值拟合一个多项式，使得现有的数据满足这个多项式，再利用多项式求缺失值
  * 拉格朗日插值
  * 牛顿插值
* 样条插值：以可变样条来作出一条经过一系列点的光滑曲线的插值方法

pandas提供了名为interpolate的插值方法：

* 线性插值：`from scipy.interpolate import interp1d`

  `interp1d(x,y1,kind='linear')`

* 多项式插值（拉格朗日插值）：`from scipy.interpolate import lagrange``

  `lagrange(x,y1)`

* 样条插值：`from scipy.interpolate import spline``

  `SplineInsValue1 = spline(x,y1,xnew=np.array([6,7]))`

```python
import numpy as np

from scipy.interpolate import interp1d

x=np.array([1,2,3,4,5,8,9,10]) ##创建自变量x
y1=np.array([2,8,18,32,50,128,162,200]) ##创建因变量y1
y2=np.array([3,5,7,9,11,17,19,21]) ##创建因变量y2

LinearInsValue1 = interp1d(x,y1,kind='linear') ##线性插值拟合x,y1
LinearInsValue2 = interp1d(x,y2,kind='linear') ##线性插值拟合x,y2

print('当x为6、7时，使用线性插值y1为：',LinearInsValue1([6,7]))
print('当x为6、7时，使用线性插值y2为：',LinearInsValue2([6,7]))
```

```python
from scipy.interpolate import lagrange

LargeInsValue1 = lagrange(x,y1) ##拉格朗日插值拟合x,y1
LargeInsValue2 = lagrange(x,y2) ##拉格朗日插值拟合x,y2

print('当x为6,7时，使用拉格朗日插值y1为：',LargeInsValue1([6,7]))
print('当x为6,7时，使用拉格朗日插值y2为：',LargeInsValue2([6,7]))
```

```python
from scipy.interpolate import spline

##样条插值拟合x,y1
SplineInsValue1 = spline(x,y1,xnew=np.array([6,7]))

##样条插值拟合x,y2
SplineInsValue2 = spline(x,y2,xnew=np.array([6,7]))
print('当x为6,7时，使用样条插值y1为：',SplineInsValue1)
print('当x为6,7时，使用样条插值y2为：',SplineInsValue2)
```

### 检测与处理异常值

主要为3σ原则和箱线图分析两种方式。

#### 3σ原则

该原则先假设一组检测数据只含有随机误差，对原始数据进行计算处理得到标准差，然后按一定的概率确定一个区间，认为误差超过这个区间就属于异常。

*注意：这种方式仅适用于对正态或近似正态分布的样本数据进行处理*

看出数据的数值几乎集中分布于区间(μ-3σ，μ+3σ)之前，认为超过3σ的部分为异常数据（其中μ为均值，σ为标准差）

```python
# 定义3σ原则来识别异常值函数
def outRange(Ser1):
    boolInd = (Ser1.mean()-3*Ser1.std()>Ser1) | (Ser1.mean()+3*Ser1.var()< Ser1)
    index = np.arange(Ser1.shape[0])[boolInd]
    outrange = Ser1.iloc[index]
    return outrange
outlier = outRange(detail['counts'])
print('使用拉依达准则判定异常值个数为:',outlier.shape[0])
print('异常值的最大值为：',outlier.max())
print('异常值的最小值为：',outlier.min())
```

#### 箱线图分析

箱线图中异常值通常被定义为小于QL-1.5IQR或大于QL+1.5IQR的值，其中QL称为下四分位数，QU为上四分位数，IQR为四分位数间距，是QU和QL之差，即全部观察值距离的一半。

*注意：箱线图没有对数据做任何限制性要求。*

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(10,8)) 

p = plt.boxplot(detail['counts'].values,notch=True)   ##画出箱线图

print(detail['counts'].values)

outlier1 = p['fliers'][0].get_ydata()   ##fliers为异常值的标签

plt.show()
print('销售量数据异常值个数为：',len(outlier1))
print('销售量数据异常值的最大值为：',max(outlier1))
print('销售量数据异常值的最小值为：',min(outlier1))
```

#### 综合练习

```python
import pandas as pd
detail = pd.read_csv('detail.csv',
    index_col=0,encoding = 'gbk')
print('进行去重操作前订单详情表的形状为：',detail.shape)

##样本去重
detail.drop_duplicates(inplace = True)


##特征去重
def FeatureEquals(df):
    ##定义求取特征是否完全相同的矩阵的函数
    dfEquals=pd.DataFrame([],columns=df.columns,index=df.columns)
    for i in df.columns:
        for j in df.columns:
            dfEquals.loc[i,j]=df.loc[:,i].equals(df.loc[:,j])
    return dfEquals

detEquals=FeatureEquals(detail)## 应用上述函数

##遍历所有数据
lenDet = detEquals.shape[0]
dupCol = []
for k in range(lenDet):
    for l in range(k+1,lenDet):
        if detEquals.iloc[k,l] & (detEquals.columns[l] not in dupCol):
            dupCol.append(detEquals.columns[l])
##删除重复列
detail.drop(dupCol,axis=1,inplace=True)
print('进行去重操作后订单详情表的形状为：',detail.shape)
```

## 标准化数据

### 离差标准化数据

将原始数据的数值映射到[0,1]区间，公式：
$$
X^*=\frac{X-min}{max-min}\
$$

```python
import pandas as pd
import numpy as np
detail = pd.read_csv('detail.csv',
    index_col=0,encoding = 'gbk')
## 自定义离差标准化函数
def MinMaxScale(data):
    data=(data-data.min())/(data.max()-data.min())
    return data
##对菜品订单表售价和销量做离差标准化
data1=MinMaxScale(detail['counts'])
data2=MinMaxScale(detail ['amounts'])
data3=pd.concat([data1,data2],axis=1)
print('离差标准化之前销量和售价数据为：\n',
    detail[['counts','amounts']].head())
print('离差标准化之后销量和售价数据为：\n',data3.head())

//output：
离差标准化之前销量和售价数据为：
            counts  amounts
detail_id                 
2956            1       49
2958            1       48
2961            1       30
2966            1       25
2968            1       13
离差标准化之后销量和售价数据为：
            counts   amounts
detail_id                  
2956          0.0  0.271186
2958          0.0  0.265537
2961          0.0  0.163842
2966          0.0  0.135593
2968          0.0  0.067797
```

发现数据之间的差值非常小，这是数据极差过大造成的。

缺点：

* 若数据集中的某个数值很多，则离差标准化的值就会接近于0，并且相互差别不大。
* 若将来遇到超过目前属性[min,max]取值范围的情况，会引起系统出错，此时需要重新确定min和max。

### 标准差标准化数据

又称零均值标准化或z分数标准化，是当前使用最广泛的数据标准化方法。

*该方法处理的数据：均值为0，标准差为1*，公式如下：
$$
X^*=\frac{X-\tilde{X}}{δ}，\tilde{X}为原始数据的均值，δ为原始数据的标准差
$$

```python
##自定义标准差标准化函数
def StandardScaler(data):
    data=(data-data.mean())/data.std()
    return data
##对菜品订单表售价和销量做标准化
data4=StandardScaler(detail['counts'])
data5=StandardScaler(detail['amounts'])
data6=pd.concat([data4,data5],axis=1)
print('标准差标准化之前销量和售价数据为：\n',
    detail[['counts','amounts']].head())
print('标准差标准化之后销量和售价数据为：\n',data6.head())
```

发现，标准差标准化数据后的值区间不限于[0,1]，并且存在负值，同时，*标准差标准化和离差标准化都不会改变数据的分布情况*。

### 小数定标标准化数据

通过移动数据的小数位数，将数据映射到区间[-1,1]，移动的小数位数取决于数据绝对值的最大值。

公式如下：
$$
X^*=\frac{X}{10^k}
$$

```python
##自定义小数定标差标准化函数
def DecimalScaler(data):
    data=data/10**np.ceil(np.log10(data.abs().max()))
    return data

##对菜品订单表售价和销量做标准化
data7=DecimalScaler(detail['counts'])
data8=DecimalScaler(detail['amounts'])
data9=pd.concat([data7,data8],axis=1)
print('小数定标标准化之前的销量和售价数据：\n',
    detail[['counts','amounts']].head())
print('小数定标标准化之后的销量和售价数据：\n',data9.head())
```

## 转换数据

### 哑变量处理类别型数据

哑变量：用以反映质的属性的一个人工变量，通常为0或1

实际数据中，特征的类型不一定为数值型，可能为类别型，此时需要进行处理才能放入模型之中。

可以利用pandas库中的get_dummies函数来对类别型特征进行哑变量处理：

```python
import pandas as pd
import numpy as np
detail = pd.read_csv('detail.csv',encoding = 'gbk')
data=detail.loc[0:5,'dishes_name']   ##抽取部分数据做演示
print('哑变量处理前的数据为：\n',data)
print('哑变量处理后的数据为：\n',pd.get_dummies(data))

//output:
哑变量处理前的数据为：
 0     蒜蓉生蚝
1    蒙古烤羊腿
2     大蒜苋菜
3    芝麻烤紫菜
4      蒜香包
5      白斩鸡
Name: dishes_name, dtype: object
哑变量处理后的数据为：
    大蒜苋菜  白斩鸡  芝麻烤紫菜  蒙古烤羊腿  蒜蓉生蚝  蒜香包
0     0    0      0      0     1    0
1     0    0      0      1     0    0
2     1    0      0      0     0    0
3     0    0      1      0     0    0
4     0    0      0      0     0    1
5     0    1      0      0     0    0
```

对于一个类别型特征，若取值为m个，经过哑变量处理后，变成m个二元特征。

### 离散化连续型数据

即将连续型特征变换成离散型特征。

常用的离散化方法：等宽法、等频法、聚类分析法。

#### 等宽法

将数据的值域分成具有相同宽度的区间。

pandas提供了cut函数，其中bins参数若接收int，代表离散化后的类别数目；若接收序列化类型的数据，表示进行切分的区间，每两个数的间隔为一个区间。

```python
price = pd.cut(detail['amounts'],5)
print('离散化后5条记录售价分布为：\n' ,price.value_counts())

//output:
离散化后5条记录售价分布为：
 (0.823, 36.4]     5461
(36.4, 71.8]      3157
(71.8, 107.2]      839
(142.6, 178.0]     426
(107.2, 142.6]     154
Name: amounts, dtype: int64
```

发现，等宽法对数据分布要求较高，若数据分布不均匀，会严重损坏所建立的模型。

#### 等频法

```python
##自定义等频法离散化函数
def SameRateCut(data,k):
    # 含头不含尾
    w=data.quantile(np.arange(0,1+1.0/k,1.0/k))
    data=pd.cut(data,w)
    return data
result=SameRateCut(detail['amounts'],5).value_counts()   ##菜品售价
print('菜品数据等频法离散化后各个类别数目分布状况为：','\n',result)

//output:
菜品数据等频法离散化后各个类别数目分布状况为： 
 (18.0, 32.0]     2107
(39.0, 58.0]     2080
(32.0, 39.0]     1910
(1.0, 18.0]      1891
(58.0, 178.0]    1863
Name: amounts, dtype: int64
```

避免了类分布不均匀的问题

#### 聚类分析法

将连续型数据用聚类算法（如Kmeans等）进行聚类，然后处理聚类得到的簇，为合并得到一个簇的连续型数据作同一种标记。

```python
#自定义数据k-Means聚类离散化函数
def KmeanCut(data,k):
    from sklearn.cluster import KMeans #引入KMeans
    kmodel=KMeans(n_clusters=k)   #建立模型
    kmodel.fit(data.values.reshape((len(data), 1)))    #训练模型
    c=pd.DataFrame(kmodel.cluster_centers_).sort_values(0)   #输出聚类中心并排序
#     print(c)
#             0
# 3   22.309501
# 0   43.509866
# 4   73.945304
# 2  131.858137
    w=c.rolling(2).mean().iloc[1:]    #相邻两项求中点，作为边界点
    print(w)
    w=[0]+list(w[0])+[data.max()]    #把首末边界点加上
    print(w)
    data=pd.cut(data,w)
    return data
#菜品售价等频法离散化
result=KmeanCut(detail['amounts'],5).value_counts()
print('菜品售价聚类离散化后各个类别数目分布状况为：','\n',result)
```

## 小结

* 数据分析的预处理过程：
  * 数据清洗
  * 数据合并
  * 数据标准化
  * 数据转换

  它们之间存在交叉，没有严格的先后关系。

* 数据清洗：

  * 重复值的处理
    * 记录去重
    * 特征去重
  * 缺失值的处理
    * 删除
    * 替换
    * 插值
  * 异常值的处理
    * 3σ原则
    * 箱线图分析

* 数据合并：

  将多个数据源中的数据合并并存放到一个数据存储的过程

* 数据标准化：

  将不同量纲的数据转化为可以相互比较的标准化数据

  * 离差标准化数据
  * 标准差标准化数据
  * 小数定标标准化数据

* 数据转换：

  从不同的应用角度对已有特征进行转换

  * 哑变量处理类别型数据
  * 离散化连续型数据
    * 等宽法
    * 等频法
    * 聚类分析法

