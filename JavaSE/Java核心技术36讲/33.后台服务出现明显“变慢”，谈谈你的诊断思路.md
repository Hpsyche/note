## 典型回答

* 问题可能来自于Java服务自身，也可能仅仅是受系统里其他服务的影响。初始判断可以先确认是否出现了意外的程序错误，例如检查应用本身的错误日志。

  对于分布式系统，很多公司都会实现更加系统的日志、性能等监控系统。一些Java诊断工具也可以用于这个诊断，例如通过JFR（Java Flight Recorder），监控应用是否大量出现了某种类型的异常。

  如果有，那么异常可能就是个突破点。

  如果没有，可以先检查系统级别的资源等情况，监控CPU、内存等资源是否被其他进程大量占用，并且这种占用是否不符合系统正常运行状况。

* 监控Java服务自身，例如GC日志里面是否观察到Full GC等恶劣情况出现，或者是否Minor GC在变长等；

  利用jstat等工具，获取内存使用的统计信息也是个常用手段；利用jstack等工具检查是否出现死锁等。

* 如果还不能确定具体问题，对应用进行Profling也是个办法，但因为它会对系统产生侵入性，如果不是非常必要，大多数情况下并不建议在生产系统进行。
* 定位了程序错误或者JVM配置的问题后，就可以采取相应的补救措施，然后验证是否解决，否则还需要重复上面部分过程。

## 问题扩展

系统性能分析中，CPU、内存和IO是主要关注项。

对于CPU，如果是常见的Linux，可以先用top命令查看负载状况，下图是我截取的一个状态：

![](D:\Work\TyporaNotes\note\JavaSE\Java核心技术36讲\pict\33-1.PNG)

可以看到，其平均负载（load average）的三个值（分别是1分钟、5分钟、15分钟）非常低，并且暂时看并没有升高迹象。如果这些数值非常高（例如，超过50%、60%），并且短期平均值高于长期平均值，则表明负载很重；如果还有升高的趋势，那么就要非常警惕了。

进一步的排查有很多思路，例如，怎么找到最耗费CPU的Java线程，简要介绍步骤：

* 利用top命令获取相应pid，“-H”代表thread模式，你可以配合grep命令更精准定位。

> top –H

* 然后转换成为16进制。

> printf "%x" your_pid

* 最后利用jstack获取的线程栈，对比相应的ID即可。

当然，还有更加通用的诊断方向，利用vmstat之类，查看上下文切换的数量，比如下面就是指定时间间隔为1，收集10次：

> vmsat -1 -10

输出如下：

![](D:\Work\TyporaNotes\note\JavaSE\Java核心技术36讲\pict\33-2.PNG)

如果每秒上下文（cs，context switch）切换很高，并且比系统中断高很多（in，system interrupt），就表明很有可能是因为不合理的多线程调度所导致。